import sqlite3
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class EnhancedDataLoader:
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á –¥–∞–Ω–∏—Ö –¥–ª—è ML —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    –ü—Ä–∞—Ü—é—î –∑ –Ω–æ–≤–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é –ë–î MLTrainingData
    """
    
    def __init__(self, db_path: str = "../MusicRecommender.db"):
        self.db_path = db_path
        self.connection = None
        
    def __enter__(self):
        self.connection = sqlite3.connect(self.db_path)
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.connection:
            self.connection.close()
    
    def get_connection(self) -> sqlite3.Connection:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ –ë–î"""
        if not self.connection:
            self.connection = sqlite3.connect(self.db_path)
        return self.connection
    
    def load_ml_training_data(self, 
                            min_interactions_per_user: int = 5,
                            include_skips: bool = False,
                            time_window_days: Optional[int] = None) -> pd.DataFrame:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ –Ω–æ–≤–æ—ó —Ç–∞–±–ª–∏—Ü—ñ MLTrainingData
        
        Args:
            min_interactions_per_user: –ú—ñ–Ω—ñ–º—É–º –≤–∑–∞—î–º–æ–¥—ñ–π –Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            include_skips: –í–∫–ª—é—á–∏—Ç–∏ —Å–∫—ñ–ø–∏ (rating < 0.3)
            time_window_days: –¢—ñ–ª—å–∫–∏ –¥–∞–Ω—ñ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ N –¥–Ω—ñ–≤
        """
        logger.info("üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ML —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")
        
        conn = self.get_connection()
        
        # –ë–∞–∑–æ–≤–∏–π –∑–∞–ø–∏—Ç
        base_query = """
        SELECT 
            UserId, SpotifyTrackId, 
            Danceability, Energy, Valence, Tempo, Acousticness, 
            Instrumentalness, Speechiness, Loudness, Popularity,
            DurationMs, [Key], Mode, TimeSignature,
            Artist, Genre, ReleaseYear, ArtistPopularity,
            Rating, InteractionType, PlayCount, PlayDuration,
            ListeningContext, Timestamp,
            UserAvgDanceability, UserAvgEnergy, UserAvgValence, UserAvgTempo
        FROM MLTrainingData
        WHERE 1=1
        """
        
        conditions = []
        params = []
        
        # –§—ñ–ª—å—Ç—Ä –∑–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–º
        if not include_skips:
            conditions.append("Rating >= ?")
            params.append(0.3)
        
        # –§—ñ–ª—å—Ç—Ä –∑–∞ —á–∞—Å–æ–º
        if time_window_days:
            cutoff_date = (datetime.now() - timedelta(days=time_window_days)).isoformat()
            conditions.append("Timestamp >= ?")
            params.append(cutoff_date)
        
        # –ó–±–∏—Ä–∞—î–º–æ –∑–∞–ø–∏—Ç
        if conditions:
            query = base_query + " AND " + " AND ".join(conditions)
        else:
            query = base_query
            
        query += " ORDER BY Timestamp DESC"
        
        df = pd.read_sql_query(query, conn, params=params)
        
        if df.empty:
            logger.warning("‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö!")
            return df
        
        logger.info(f"üìà –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö")
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –≤–∑–∞—î–º–æ–¥—ñ–π
        user_counts = df['UserId'].value_counts()
        valid_users = user_counts[user_counts >= min_interactions_per_user].index
        df_filtered = df[df['UserId'].isin(valid_users)]
        
        logger.info(f"üë• –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {df_filtered['UserId'].nunique()}")
        logger.info(f"üéµ –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç—Ä–µ–∫—ñ–≤: {df_filtered['SpotifyTrackId'].nunique()}")
        
        return df_filtered
    
    def load_user_profiles(self) -> pd.DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ML –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤"""
        logger.info("üë§ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ML –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤...")
        
        conn = self.get_connection()
        
        query = """
        SELECT 
            UserId, PreferredDanceability, PreferredEnergy, PreferredValence, PreferredTempo,
            PreferredAcousticness, PreferredInstrumentalness, PreferredSpeechiness, PreferredLoudness,
            DanceabilityVariance, EnergyVariance, ValenceVariance, TempoVariance,
            SkipRate, RepeatRate, ExplorationRate,
            GenreDiversity, ArtistDiversity, ClusterId,
            TotalInteractions, LastUpdated
        FROM MLUserProfiles
        """
        
        df = pd.read_sql_query(query, conn)
        logger.info(f"üë• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤")
        
        return df
    
    def prepare_content_based_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è content-based –º–æ–¥–µ–ª—ñ
        
        Returns:
            X: –§—ñ—á—ñ —Ç—Ä–µ–∫—ñ–≤ + –∫–æ–Ω—Ç–µ–∫—Å—Ç
            y: –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ (—Ä–µ–π—Ç–∏–Ω–≥)
        """
        logger.info("üéØ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è content-based –º–æ–¥–µ–ª—ñ...")
        
        # –ê—É–¥—ñ–æ —Ñ—ñ—á—ñ —Ç—Ä–µ–∫—É
        audio_features = [
            'Danceability', 'Energy', 'Valence', 'Tempo', 'Acousticness',
            'Instrumentalness', 'Speechiness', 'Loudness', 'Popularity'
        ]
        
        # –ö–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫—ñ —Ñ—ñ—á—ñ
        user_features = [
            'UserAvgDanceability', 'UserAvgEnergy', 'UserAvgValence', 'UserAvgTempo'
        ]
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ —Ñ—ñ—á—ñ
        context_features = self._extract_context_features(df)
        
        # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Ñ—ñ—á—ñ
        X = df[audio_features + user_features].copy()
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ —Ñ—ñ—á—ñ
        for col, values in context_features.items():
            X[col] = values
            
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è Tempo —Ç–∞ Loudness
        X['Tempo_norm'] = (X['Tempo'] - X['Tempo'].min()) / (X['Tempo'].max() - X['Tempo'].min())
        X['Loudness_norm'] = (X['Loudness'] - X['Loudness'].min()) / (X['Loudness'].max() - X['Loudness'].min())
        
        # –ó–∞–ø–æ–≤–Ω—é—î–º–æ NaN
        X = X.fillna(0)
        
        # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
        y = df['Rating']
        
        logger.info(f"üìä –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {X.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤ –∑ {X.shape[1]} —Ñ—ñ—á–∞–º–∏")
        
        return X, y
    
    def prepare_collaborative_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è collaborative filtering
        
        Returns:
            user_item_matrix: –ú–∞—Ç—Ä–∏—Ü—è user-item –∑ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏
        """
        logger.info("üë• –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è collaborative filtering...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Ç—Ä–∏—Ü—é user-item
        user_item_matrix = df.pivot_table(
            index='UserId',
            columns='SpotifyTrackId', 
            values='Rating',
            fill_value=0
        )
        
        logger.info(f"üìä –ú–∞—Ç—Ä–∏—Ü—è {user_item_matrix.shape[0]} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ x {user_item_matrix.shape[1]} —Ç—Ä–µ–∫—ñ–≤")
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω–æ—Å—Ç—ñ
        sparsity = 1 - (df.shape[0] / (user_item_matrix.shape[0] * user_item_matrix.shape[1]))
        logger.info(f"üîç –†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ñ: {sparsity:.3f}")
        
        return user_item_matrix
    
    def _extract_context_features(self, df: pd.DataFrame) -> Dict[str, List]:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—Ö —Ñ—ñ—á–µ–π"""
        context_features = {}
        
        # –ß–∞—Å –¥–æ–±–∏ (one-hot encoding)
        time_contexts = ['morning', 'afternoon', 'evening', 'night']
        for context in time_contexts:
            context_features[f'context_{context}'] = (df['ListeningContext'] == context).astype(int)
        
        # –†—ñ–∫ –≤–∏–ø—É—Å–∫—É (–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π)
        if 'ReleaseYear' in df.columns:
            min_year = df['ReleaseYear'].min()
            max_year = df['ReleaseYear'].max()
            if max_year > min_year:
                context_features['release_year_norm'] = (df['ReleaseYear'] - min_year) / (max_year - min_year)
            else:
                context_features['release_year_norm'] = [0.5] * len(df)
        
        # –¢–∏–ø –≤–∑–∞—î–º–æ–¥—ñ—ó (one-hot)
        interaction_types = [1, 2, 3, 4]  # Like, Skip, Play, Save
        for int_type in interaction_types:
            context_features[f'interaction_type_{int_type}'] = (df['InteractionType'] == int_type).astype(int)
        
        return context_features
    
    def get_user_interaction_history(self, user_id: int, limit: int = 100) -> pd.DataFrame:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤–∑–∞—î–º–æ–¥—ñ–π –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"""
        conn = self.get_connection()
        
        query = """
        SELECT * FROM MLTrainingData 
        WHERE UserId = ? 
        ORDER BY Timestamp DESC 
        LIMIT ?
        """
        
        df = pd.read_sql_query(query, conn, params=[user_id, limit])
        return df
    
    def get_track_features_for_prediction(self, track_ids: List[str]) -> pd.DataFrame:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ñ—ñ—á–µ–π —Ç—Ä–µ–∫—ñ–≤ –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ü—ñ—ó"""
        if not track_ids:
            return pd.DataFrame()
            
        conn = self.get_connection()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –¥–ª—è SQL –∑–∞–ø–∏—Ç—É
        placeholders = ','.join(['?' for _ in track_ids])
        
        query = f"""
        SELECT DISTINCT
            sf.SpotifyTrackId,
            sf.Danceability, sf.Energy, sf.Valence, sf.Tempo, sf.Acousticness,
            sf.Instrumentalness, sf.Speechiness, sf.Loudness, sf.Popularity,
            sf.DurationMs, sf.[Key], sf.Mode, sf.TimeSignature,
            h.Artist, sf.Genre, h.ReleaseYear, sf.ArtistPopularity
        FROM SongFeatures sf
        LEFT JOIN History h ON sf.SpotifyTrackId = h.SpotifyTrackId
        WHERE sf.SpotifyTrackId IN ({placeholders})
        """
        
        df = pd.read_sql_query(query, conn, params=track_ids)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        if not df.empty:
            df['Tempo_norm'] = (df['Tempo'] - df['Tempo'].min()) / (df['Tempo'].max() - df['Tempo'].min())
            df['Loudness_norm'] = (df['Loudness'] - df['Loudness'].min()) / (df['Loudness'].max() - df['Loudness'].min())
            df = df.fillna(0)
        
        return df
    
    def save_model_metrics(self, model_type: str, model_version: str, metrics: Dict) -> None:
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª—ñ –≤ –ë–î"""
        conn = self.get_connection()
        
        insert_query = """
        INSERT INTO MLModelMetrics (
            ModelType, ModelVersion, Accuracy, Precision, Recall, F1Score, MAE, MSE,
            TrainingSamples, TestSamples, UniqueUsers, UniqueTracks,
            TrainingDate, TrainingDuration, ModelConfig, FeatureImportance
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        
        training_duration_seconds = metrics.get('training_duration', 0)
        training_duration_str = f"{training_duration_seconds:.2f} seconds"
        
        params = [
            model_type,
            model_version,
            metrics.get('accuracy', 0.0),
            metrics.get('precision', 0.0),
            metrics.get('recall', 0.0),
            metrics.get('f1_score', 0.0),
            metrics.get('mae', 0.0),
            metrics.get('mse', 0.0),
            metrics.get('training_samples', 0),
            metrics.get('test_samples', 0),
            metrics.get('unique_users', 0),
            metrics.get('unique_tracks', 0),
            datetime.now().isoformat(),
            training_duration_str,
            json.dumps(metrics.get('config', {})),
            json.dumps(metrics.get('feature_importance', {}))
        ]
        
        conn.execute(insert_query, params)
        conn.commit()
        
        logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª—ñ {model_type} v{model_version}")
    
    def get_latest_model_metrics(self, model_type: str) -> Optional[Dict]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª—ñ"""
        conn = self.get_connection()
        
        query = """
        SELECT * FROM MLModelMetrics 
        WHERE ModelType = ? 
        ORDER BY TrainingDate DESC 
        LIMIT 1
        """
        
        cursor = conn.execute(query, [model_type])
        row = cursor.fetchone()
        
        if row:
            columns = [description[0] for description in cursor.description]
            return dict(zip(columns, row))
        
        return None
    
    def cleanup_old_metrics(self, keep_last_n: int = 10) -> int:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –º–µ—Ç—Ä–∏–∫ (–∑–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ N –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É –º–æ–¥–µ–ª—ñ)"""
        conn = self.get_connection()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ ID –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è
        query = """
        WITH RankedMetrics AS (
            SELECT Id, ModelType,
                   ROW_NUMBER() OVER (PARTITION BY ModelType ORDER BY TrainingDate DESC) as rn
            FROM MLModelMetrics
        )
        SELECT Id FROM RankedMetrics WHERE rn > ?
        """
        
        cursor = conn.execute(query, [keep_last_n])
        ids_to_delete = [row[0] for row in cursor.fetchall()]
        
        if ids_to_delete:
            placeholders = ','.join(['?' for _ in ids_to_delete])
            delete_query = f"DELETE FROM MLModelMetrics WHERE Id IN ({placeholders})"
            conn.execute(delete_query, ids_to_delete)
            conn.commit()
            
            logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ {len(ids_to_delete)} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –º–µ—Ç—Ä–∏–∫")
        
        return len(ids_to_delete)
    
    def get_training_data_stats(self) -> Dict:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        conn = self.get_connection()
        
        stats = {}
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor = conn.execute("SELECT COUNT(*) FROM MLTrainingData")
        stats['total_interactions'] = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(DISTINCT UserId) FROM MLTrainingData")
        stats['unique_users'] = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(DISTINCT SpotifyTrackId) FROM MLTrainingData")
        stats['unique_tracks'] = cursor.fetchone()[0]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞—Ö –≤–∑–∞—î–º–æ–¥—ñ–π
        cursor = conn.execute("""
            SELECT InteractionType, COUNT(*) 
            FROM MLTrainingData 
            GROUP BY InteractionType
        """)
        stats['interaction_types'] = dict(cursor.fetchall())
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥–∞—Ö
        cursor = conn.execute("""
            SELECT 
                AVG(Rating) as avg_rating,
                MIN(Rating) as min_rating,
                MAX(Rating) as max_rating
            FROM MLTrainingData
        """)
        rating_stats = cursor.fetchone()
        stats['rating_stats'] = {
            'avg': rating_stats[0],
            'min': rating_stats[1],
            'max': rating_stats[2]
        }
        
        return stats 