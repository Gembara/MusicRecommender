import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib
import logging
from typing import List, Dict, Tuple
from data_loader import DataLoader
import os
from scipy.sparse import csr_matrix
from scipy.spatial.distance import cosine

logger = logging.getLogger(__name__)

class MusicRecommenderML:
    def __init__(self):
        self.data_loader = DataLoader()
        self.content_model = None  # Content-based –º–æ–¥–µ–ª—å
        self.collaborative_model = None  # Collaborative filtering –º–æ–¥–µ–ª—å
        self.svd_model = None  # SVD –º–æ–¥–µ–ª—å
        self.scaler = StandardScaler()
        self.feature_columns = [
            'Danceability', 'Energy', 'Valence', 'Tempo_norm',
            'Acousticness', 'Instrumentalness', 'Speechiness',
            'Loudness_norm', 'Popularity'
        ]
        self.is_trained = False
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
        self.user_item_matrix = None
        self.user_item_matrix_normalized = None
        self.user_means = None
        self.item_means = None
        self.global_mean = None
        self.svd_user_factors = None
        self.svd_item_factors = None
        self.user_id_to_idx = {}
        self.item_id_to_idx = {}
        self.idx_to_user_id = {}
        self.idx_to_item_id = {}
        
    def train_models(self) -> Dict[str, float]:
        """
        –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏:
        1. Content-Based: Random Forest –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ü—ñ—ó —Ä–µ–π—Ç–∏–Ω–≥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞—É–¥—ñ–æ —Ñ—ñ—á–µ–π
        2. Collaborative: –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π KNN –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ weighted similarities
        3. SVD: –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—á–Ω–∞ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –∑ bias terms
        """
        logger.info("üéØ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö ML –º–æ–¥–µ–ª–µ–π...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        training_data, all_features = self.data_loader.prepare_training_data()
        
        if training_data.empty:
            logger.error("‚ùå –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è!")
            return {"error": "–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"}
        
        logger.info(f"üìä –î–∞–Ω—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {len(training_data)} –∑–∞–ø–∏—Å—ñ–≤")
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑–æ–≤–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–∏—Ö
        self._prepare_user_item_mappings(training_data)
        
        # 1. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Content-Based –º–æ–¥–µ–ª—ñ
        content_metrics = self._train_content_based_model(training_data)
        
        # 2. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó Collaborative Filtering –º–æ–¥–µ–ª—ñ
        collaborative_metrics = self._train_improved_collaborative_model(training_data)
        
        # 3. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó SVD –º–æ–¥–µ–ª—ñ
        svd_metrics = self._train_improved_svd_model(training_data)
        
        self.is_trained = True
        
        metrics = {
            **content_metrics,
            **collaborative_metrics,
            **svd_metrics,
            "total_training_samples": len(training_data),
            "unique_users": training_data['UserId'].nunique(),
            "unique_tracks": training_data['SpotifyTrackId'].nunique()
        }
        
        logger.info("‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
        return metrics
    
    def _prepare_user_item_mappings(self, data: pd.DataFrame):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —ñ–Ω–¥–µ–∫—Å–Ω–∏—Ö –º–∞–ø—ñ–Ω–≥—ñ–≤ –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —Ç–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤"""
        unique_users = sorted(data['UserId'].unique())
        unique_items = sorted(data['SpotifyTrackId'].unique())
        
        self.user_id_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
        self.item_id_to_idx = {item_id: idx for idx, item_id in enumerate(unique_items)}
        self.idx_to_user_id = {idx: user_id for user_id, idx in self.user_id_to_idx.items()}
        self.idx_to_item_id = {idx: item_id for item_id, idx in self.item_id_to_idx.items()}
        
        logger.info(f"üìã –ú–∞–ø—ñ–Ω–≥–∏ —Å—Ç–≤–æ—Ä–µ–Ω—ñ: {len(unique_users)} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, {len(unique_items)} —Ç—Ä–µ–∫—ñ–≤")

    def _train_content_based_model(self, data: pd.DataFrame) -> Dict[str, float]:
        """–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Content-Based –º–æ–¥–µ–ª—ñ"""
        logger.info("üéµ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Content-Based –º–æ–¥–µ–ª—ñ...")
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ—á–µ–π
        X = data[self.feature_columns].fillna(0)
        y = data['Rating'].values
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ—á–µ–π
        X_scaled = self.scaler.fit_transform(X)
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —Ç–∞ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Random Forest
        self.content_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.content_model.fit(X_train, y_train)
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
        y_pred = self.content_model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        
        # –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ñ—ñ—á–µ–π
        feature_importance = dict(zip(
            self.feature_columns,
            self.content_model.feature_importances_
        ))
        
        logger.info(f"üìà Content Model - MSE: {mse:.3f}, MAE: {mae:.3f}")
        logger.info(f"üîç –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ —Ñ—ñ—á—ñ: {sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]}")
        
        return {
            "content_mse": mse,
            "content_mae": mae,
            "content_feature_importance": feature_importance
        }
    
    def _train_improved_collaborative_model(self, data: pd.DataFrame) -> Dict[str, float]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ Collaborative Filtering –º–æ–¥–µ–ª—å –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ weighted similarities"""
        logger.info("üë• –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó Collaborative Filtering –º–æ–¥–µ–ª—ñ...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–∫—Ä–∞—â–µ–Ω—É user-item –º–∞—Ç—Ä–∏—Ü—é
        n_users = len(self.user_id_to_idx)
        n_items = len(self.item_id_to_idx)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω—É –º–∞—Ç—Ä–∏—Ü—é –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        user_indices = [self.user_id_to_idx[uid] for uid in data['UserId']]
        item_indices = [self.item_id_to_idx[iid] for iid in data['SpotifyTrackId']]
        ratings = data['Rating'].values
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —â—ñ–ª—å–Ω—É –º–∞—Ç—Ä–∏—Ü—é –¥–ª—è KNN (–Ω–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤)
        self.user_item_matrix = np.zeros((n_users, n_items))
        for u_idx, i_idx, rating in zip(user_indices, item_indices, ratings):
            self.user_item_matrix[u_idx, i_idx] = rating
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ —Ä–µ–π—Ç–∏–Ω–≥–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
        self.user_means = np.array([
            np.mean(self.user_item_matrix[u][self.user_item_matrix[u] > 0]) 
            if np.any(self.user_item_matrix[u] > 0) else 3.0 
            for u in range(n_users)
        ])
        
        self.global_mean = np.mean(ratings)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—É –º–∞—Ç—Ä–∏—Ü—é (–≤—ñ–¥–Ω—ñ–º–∞—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ —Ä–µ–π—Ç–∏–Ω–≥–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤)
        self.user_item_matrix_normalized = self.user_item_matrix.copy()
        for u_idx in range(n_users):
            mask = self.user_item_matrix[u_idx] > 0
            self.user_item_matrix_normalized[u_idx][mask] -= self.user_means[u_idx]
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó KNN –º–æ–¥–µ–ª—ñ –∑ pearson correlation
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ –º—ñ–Ω—ñ–º—É–º 2 —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏
        active_users_mask = np.sum(self.user_item_matrix > 0, axis=1) >= 2
        self.active_user_indices = np.where(active_users_mask)[0]
        
        if len(self.active_user_indices) < 2:
            logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∞–∫—Ç–∏–≤–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –¥–ª—è collaborative filtering")
            self.collaborative_model = None
            return {"collaborative_error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö"}
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—É –º–∞—Ç—Ä–∏—Ü—é –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è KNN
        active_user_matrix = self.user_item_matrix_normalized[self.active_user_indices]
        
        # –ö–∞—Å—Ç–æ–º–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–∂–æ—Å—Ç—ñ (adjusted cosine similarity)
        self.collaborative_model = NearestNeighbors(
            n_neighbors=min(10, len(self.active_user_indices)),
            metric='cosine',
            algorithm='brute'
        )
        
        # –ó–∞–º—ñ–Ω—é—î–º–æ –Ω—É–ª—ñ –Ω–∞ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è cosine similarity
        filled_matrix = active_user_matrix.copy()
        for i in range(len(filled_matrix)):
            mask = active_user_matrix[i] == 0
            filled_matrix[i][mask] = 0  # –ó–∞–ª–∏—à–∞—î–º–æ –Ω—É–ª—ñ, –∞–ª–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ weights
        
        self.collaborative_model.fit(filled_matrix)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –∫—Ä–∞—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
        self.active_user_matrix = active_user_matrix
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        sparsity = 1 - (np.count_nonzero(self.user_item_matrix) / (n_users * n_items))
        avg_user_ratings = np.mean([np.sum(self.user_item_matrix[u] > 0) for u in range(n_users)])
        
        logger.info(f"üîç –ü–æ–∫—Ä–∞—â–µ–Ω–∞ Collaborative Model:")
        logger.info(f"   üìä –†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ—Å—Ç—å: {sparsity:.3f}")
        logger.info(f"   üë§ –ê–∫—Ç–∏–≤–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {len(self.active_user_indices)}")
        logger.info(f"   üìà –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤ –Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: {avg_user_ratings:.1f}")
        
        return {
            "collaborative_sparsity": sparsity,
            "collaborative_active_users": len(self.active_user_indices),
            "collaborative_avg_ratings_per_user": avg_user_ratings,
            "collaborative_normalization": "user_mean_centered"
        }
    
    def _train_improved_svd_model(self, data: pd.DataFrame) -> Dict[str, float]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ SVD –º–æ–¥–µ–ª—å –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –º–∞—Ç—Ä–∏—á–Ω–æ—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—î—é —Ç–∞ bias terms"""
        logger.info("üîÑ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó SVD –º–æ–¥–µ–ª—ñ...")
        
        n_users = len(self.user_id_to_idx)
        n_items = len(self.item_id_to_idx)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É user-item –º–∞—Ç—Ä–∏—Ü—é (users x items)
        user_item_dense = self.user_item_matrix.copy()
        
        # –ó–∞–ø–æ–≤–Ω—é—î–º–æ –Ω—É–ª—ñ —Å–µ—Ä–µ–¥–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –¥–ª—è SVD
        filled_matrix = user_item_dense.copy()
        for u_idx in range(n_users):
            user_ratings = user_item_dense[u_idx]
            rated_items = user_ratings > 0
            if np.any(rated_items):
                user_mean = np.mean(user_ratings[rated_items])
                filled_matrix[u_idx][~rated_items] = user_mean
            else:
                filled_matrix[u_idx][:] = self.global_mean
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –º–∞—Ç—Ä–∏—Ü—é –≤—ñ–¥–Ω—ñ–º–∞—é—á–∏ –≥–ª–æ–±–∞–ª—å–Ω–∏–π —Å–µ—Ä–µ–¥–Ω—ñ–π + user bias + item bias
        self.item_means = np.array([
            np.mean(filled_matrix[:, i][filled_matrix[:, i] > 0]) 
            if np.any(filled_matrix[:, i] > 0) else self.global_mean 
            for i in range(n_items)
        ])
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ bias-corrected –º–∞—Ç—Ä–∏—Ü—é
        bias_corrected_matrix = filled_matrix.copy()
        for u_idx in range(n_users):
            for i_idx in range(n_items):
                if user_item_dense[u_idx, i_idx] > 0:  # —Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω–∏—Ö —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤
                    bias_corrected_matrix[u_idx, i_idx] = (
                        filled_matrix[u_idx, i_idx] - self.global_mean - 
                        (self.user_means[u_idx] - self.global_mean) - 
                        (self.item_means[i_idx] - self.global_mean)
                    )
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
        max_components = min(50, min(n_users, n_items) - 1, 
                           int(np.sqrt(np.count_nonzero(user_item_dense))))
        n_components = max(5, max_components)
        
        # –¢—Ä–µ–Ω—É—î–º–æ SVD –Ω–∞ bias-corrected –º–∞—Ç—Ä–∏—Ü—ñ
        self.svd_model = TruncatedSVD(
            n_components=n_components,
            random_state=42,
            n_iter=10,
            algorithm='randomized'
        )
        
        # –§—ñ—Ç—É—î–º–æ SVD –Ω–∞ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–æ–≤–∞–Ω—ñ–π –º–∞—Ç—Ä–∏—Ü—ñ (–¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ)
        U_reduced = self.svd_model.fit_transform(bias_corrected_matrix)
        Vt_reduced = self.svd_model.components_
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ
        self.svd_user_factors = U_reduced  # [n_users, n_components]
        self.svd_item_factors = Vt_reduced.T  # [n_items, n_components]
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —è–∫—ñ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
        reconstructed = U_reduced @ Vt_reduced
        mask = user_item_dense > 0
        if np.any(mask):
            mse_reconstruction = np.mean((bias_corrected_matrix[mask] - reconstructed[mask]) ** 2)
        else:
            mse_reconstruction = 0.0
        
        explained_variance_ratio = self.svd_model.explained_variance_ratio_.sum()
        
        logger.info(f"üîç –ü–æ–∫—Ä–∞—â–µ–Ω–∞ SVD Model:")
        logger.info(f"   üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤: {n_components}")
        logger.info(f"   üìä –ü–æ—è—Å–Ω–µ–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è: {explained_variance_ratio:.3f}")
        logger.info(f"   üéØ MSE —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó: {mse_reconstruction:.3f}")
        logger.info(f"   ‚öñÔ∏è –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è bias correction")
        
        return {
            "svd_components": n_components,
            "svd_explained_variance": explained_variance_ratio,
            "svd_reconstruction_mse": mse_reconstruction,
            "svd_users": n_users,
            "svd_items": n_items,
            "svd_bias_correction": True
        }
    
    def get_content_recommendations(self, user_id: int, limit: int = 20) -> List[Dict]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–Ω—Ç—É (–∞—É–¥—ñ–æ —Ñ—ñ—á—ñ)"""
        if not self.is_trained or self.content_model is None:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞!")
            return []
        
        logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è content-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ—Ñ—ñ–ª—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        user_profile = self.data_loader.get_user_profile(user_id)
        if not user_profile:
            logger.warning(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Ñ—ñ–ª—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")
            return []
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ —Ç—Ä–µ–∫–∏
        all_features = self.data_loader.load_song_features()
        if all_features.empty:
            return []
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç—Ä–µ–∫–∏, —è–∫—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–∂–µ —Å–ª—É—Ö–∞–≤ (–∑ —É—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª)
        user_interactions = self.data_loader.load_user_interactions()
        user_tracks = user_interactions[user_interactions['UserId'] == user_id]['SpotifyTrackId'].unique()
        listened_tracks = set(user_tracks)
        
        logger.info(f"–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á {user_id} –º–∞—î {len(listened_tracks)} —Ç—Ä–µ–∫—ñ–≤ –≤ —ñ—Å—Ç–æ—Ä—ñ—ó: {list(listened_tracks)[:5]}...")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–æ–≤—ñ —Ç—Ä–µ–∫–∏ (—è–∫—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –ù–ï —Å–ª—É—Ö–∞–≤)
        new_tracks = all_features[~all_features['SpotifyTrackId'].isin(listened_tracks)]
        
        logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(new_tracks)} –Ω–æ–≤–∏—Ö —Ç—Ä–µ–∫—ñ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó")
        
        if new_tracks.empty:
            logger.warning("‚ùå –ù–µ–º–∞—î –Ω–æ–≤–∏—Ö —Ç—Ä–µ–∫—ñ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó")
            return []
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ—ñ—á—ñ –Ω–æ–≤–∏—Ö —Ç—Ä–µ–∫—ñ–≤
        new_tracks = new_tracks.copy()
        new_tracks['Tempo_norm'] = (new_tracks['Tempo'] - new_tracks['Tempo'].min()) / (new_tracks['Tempo'].max() - new_tracks['Tempo'].min())
        new_tracks['Loudness_norm'] = (new_tracks['Loudness'] - new_tracks['Loudness'].min()) / (new_tracks['Loudness'].max() - new_tracks['Loudness'].min())
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ—á–µ–π –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ü—ñ—ó
        X_new = new_tracks[self.feature_columns].fillna(0)
        X_new_scaled = self.scaler.transform(X_new)
        
        # –ü—Ä–µ–¥–∏–∫—Ü—ñ—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤
        predicted_ratings = self.content_model.predict(X_new_scaled)
        
        # –î–æ–¥–∞—î–º–æ –ø—Ä–µ–¥–∏–∫–æ–≤–∞–Ω—ñ —Ä–µ–π—Ç–∏–Ω–≥–∏
        new_tracks = new_tracks.copy()
        new_tracks['predicted_rating'] = predicted_ratings
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –ø—Ä–µ–¥–∏–∫–æ–≤–∞–Ω–∏–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º
        recommendations = new_tracks.nlargest(limit, 'predicted_rating')
        
        # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        result = []
        for _, track in recommendations.iterrows():
            result.append({
                'track_id': track['SpotifyTrackId'],
                'title': track.get('Title', 'Unknown Track'),
                'artist': track.get('Artist', 'Unknown Artist'),
                'Title': track.get('Title', 'Unknown Track'),  # –î–æ–¥–∞—î–º–æ –∑ –≤–µ–ª–∏–∫–æ—é –ª—ñ—Ç–µ—Ä–æ—é
                'Artist': track.get('Artist', 'Unknown Artist'),  # –î–æ–¥–∞—î–º–æ –∑ –≤–µ–ª–∏–∫–æ—é –ª—ñ—Ç–µ—Ä–æ—é
                'Genre': track.get('Genre', 'Unknown Genre'),  # –î–æ–¥–∞—î–º–æ –∑ –≤–µ–ª–∏–∫–æ—é –ª—ñ—Ç–µ—Ä–æ—é
                'predicted_rating': float(track['predicted_rating']),
                'reason': 'Content-Based: —Å—Ö–æ–∂—ñ –∞—É–¥—ñ–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏',
                'algorithm': 'Content',
                'features': {
                    'danceability': float(track['Danceability']),
                    'energy': float(track['Energy']),
                    'valence': float(track['Valence']),
                    'genre': track.get('Genre', 'Unknown Genre')
                }
            })
        
        logger.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(result)} content-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π")
        return result
    
    def get_collaborative_recommendations(self, user_id: int, limit: int = 20) -> List[Dict]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω—ñ KNN Collaborative Filtering —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ weighted similarities"""
        if not self.is_trained or self.collaborative_model is None:
            logger.warning("‚ö†Ô∏è –ü–æ–∫—Ä–∞—â–µ–Ω–∞ collaborative –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞!")
            return []

        logger.info(f"üë• –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö collaborative —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")

        try:
            if user_id not in self.user_id_to_idx:
                logger.warning(f"‚ùå –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á {user_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∏–π —É —Å–∏—Å—Ç–µ–º—ñ")
                return self._get_popular_items_fallback(limit)

            user_idx = self.user_id_to_idx[user_id]
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —Å–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–Ω–∏—Ö
            if user_idx not in self.active_user_indices:
                logger.warning(f"‚ùå –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á {user_id} –Ω–µ —î –∞–∫—Ç–∏–≤–Ω–∏–º")
                return self._get_popular_items_fallback(limit)

            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω–¥–µ–∫—Å —É –∞–∫—Ç–∏–≤–Ω—ñ–π –º–∞—Ç—Ä–∏—Ü—ñ
            active_idx = np.where(self.active_user_indices == user_idx)[0][0]
            user_profile = self.active_user_matrix[active_idx:active_idx+1]

            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
            distances, neighbor_indices = self.collaborative_model.kneighbors(
                user_profile, 
                n_neighbors=min(8, len(self.active_user_matrix))
            )
            
            # –í–∏–∫–ª—é—á–∞—î–º–æ —Å–∞–º–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            neighbor_indices = neighbor_indices[0][1:]
            neighbor_distances = distances[0][1:]

            if len(neighbor_indices) == 0:
                logger.warning(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –¥–ª—è {user_id}")
                return self._get_popular_items_fallback(limit)

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ distances –≤ similarities (adjusted)
            neighbor_similarities = []
            for dist in neighbor_distances:
                if dist == 0:
                    sim = 1.0
                else:
                    sim = 1 / (1 + dist)  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ cosine distance –≤ similarity
                neighbor_similarities.append(sim)

            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç—Ä–µ–∫–∏ —è–∫—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–∂–µ —Å–ª—É—Ö–∞–≤
            user_interactions = self.data_loader.load_user_interactions()
            user_tracks = user_interactions[user_interactions['UserId'] == user_id]['SpotifyTrackId'].unique()
            listened_tracks = set(user_tracks)

            # –û–±—á–∏—Å–ª—é—î–º–æ weighted —Ä–µ–π—Ç–∏–Ω–≥–∏ –≤—ñ–¥ —Å—É—Å—ñ–¥—ñ–≤
            item_scores = {}
            
            for neighbor_active_idx, similarity in zip(neighbor_indices, neighbor_similarities):
                neighbor_user_idx = self.active_user_indices[neighbor_active_idx]
                neighbor_ratings = self.user_item_matrix[neighbor_user_idx]
                neighbor_mean = self.user_means[neighbor_user_idx]
                
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ç—Ä–µ–∫–∏ –∑ –≤–∏—Å–æ–∫–∏–º–∏ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ —É —Å—É—Å—ñ–¥–∞
                for item_idx, rating in enumerate(neighbor_ratings):
                    if rating >= 4.0:  # —Ç—ñ–ª—å–∫–∏ –≤–∏—Å–æ–∫–æ–æ—Ü—ñ–Ω–µ–Ω—ñ —Ç—Ä–µ–∫–∏
                        item_id = self.idx_to_item_id[item_idx]
                        
                        if item_id not in listened_tracks:
                            if item_id not in item_scores:
                                item_scores[item_id] = {
                                    'weighted_sum': 0.0,
                                    'similarity_sum': 0.0,
                                    'neighbor_count': 0,
                                    'max_rating': 0.0,
                                    'ratings': []
                                }
                            
                            # Weighted rating –∑ –≤—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º bias
                            adjusted_rating = rating - neighbor_mean + self.user_means[user_idx]
                            weighted_rating = adjusted_rating * similarity
                            
                            item_scores[item_id]['weighted_sum'] += weighted_rating
                            item_scores[item_id]['similarity_sum'] += similarity
                            item_scores[item_id]['neighbor_count'] += 1
                            item_scores[item_id]['max_rating'] = max(item_scores[item_id]['max_rating'], rating)
                            item_scores[item_id]['ratings'].append(rating)

            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º —Å–∫–æ—Ä–∏–Ω–≥–æ–º
            recommendations = []
            song_features = self.data_loader.load_song_features()
            features_dict = song_features.set_index('SpotifyTrackId').to_dict('index')

            for item_id, score_data in item_scores.items():
                if score_data['similarity_sum'] > 0:
                    # –ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ —Ä–µ–π—Ç–∏–Ω–≥—É
                    base_prediction = score_data['weighted_sum'] / score_data['similarity_sum']
                    
                    # –ë–æ–Ω—É—Å–∏ —Ç–∞ —à—Ç—Ä–∞—Ñ–∏
                    neighbor_bonus = min(0.5, score_data['neighbor_count'] * 0.1)
                    diversity_bonus = 0.2 if score_data['neighbor_count'] >= 3 else 0
                    confidence_penalty = -0.3 if score_data['similarity_sum'] < 0.5 else 0
                    
                    final_prediction = base_prediction + neighbor_bonus + diversity_bonus + confidence_penalty
                    final_prediction = max(1.0, min(5.0, final_prediction))
                    
                    track_info = features_dict.get(item_id, {})
                    
                    recommendation = {
                        'track_id': item_id,
                        'title': track_info.get('Title', 'Unknown Track'),
                        'artist': track_info.get('Artist', 'Unknown Artist'),
                        'genre': track_info.get('Genre', 'Unknown Genre'),
                        'Title': track_info.get('Title', 'Unknown Track'),
                        'Artist': track_info.get('Artist', 'Unknown Artist'),
                        'Genre': track_info.get('Genre', 'Unknown Genre'),
                        'predicted_rating': float(final_prediction),
                        'reason': f'KNN: {score_data["neighbor_count"]} —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (weighted)',
                        'algorithm': 'Improved_KNN',
                        'neighbor_count': score_data['neighbor_count'],
                        'weighted_average': float(base_prediction),
                        'max_neighbor_rating': float(score_data['max_rating']),
                        'confidence': min(0.95, score_data['similarity_sum']),
                        'neighbor_similarities': neighbor_similarities[:score_data['neighbor_count']],
                        'bias_corrected': True,
                        'similarity_sum': float(score_data['similarity_sum'])
                    }
                    
                    recommendations.append(recommendation)

            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ predicted_rating —Ç–∞ confidence
            recommendations.sort(key=lambda x: (x['predicted_rating'], x['confidence']), reverse=True)
            
            logger.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(recommendations)} –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö collaborative —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π")
            return recommendations[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö collaborative —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π: {e}")
            return self._get_popular_items_fallback(limit)
    
    def get_svd_recommendations(self, user_id: int, limit: int = 20) -> List[Dict]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω—ñ SVD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –º–∞—Ç—Ä–∏—á–Ω–æ—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—î—é —Ç–∞ bias correction"""
        if not self.is_trained or self.svd_model is None:
            logger.warning("‚ö†Ô∏è –ü–æ–∫—Ä–∞—â–µ–Ω–∞ SVD –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞!")
            return []
        
        logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö SVD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")
        
        try:
            if user_id not in self.user_id_to_idx:
                logger.warning(f"‚ùå –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á {user_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∏–π —É —Å–∏—Å—Ç–µ–º—ñ")
                return self._get_popular_items_fallback(limit)
            
            user_idx = self.user_id_to_idx[user_id]
            user_mean = self.user_means[user_idx]
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            user_latent_profile = self.svd_user_factors[user_idx]  # [n_components]
            
            # –û–±—á–∏—Å–ª—é—î–º–æ —Ä–µ–π—Ç–∏–Ω–≥–∏ –¥–ª—è –≤—Å—ñ—Ö —Ç—Ä–µ–∫—ñ–≤ —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—á–Ω—É —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é
            predicted_ratings = []
            
            for item_idx in range(len(self.idx_to_item_id)):
                item_latent = self.svd_item_factors[item_idx]  # [n_components]
                
                # SVD prediction: bias + user_factors √ó item_factors
                raw_prediction = np.dot(user_latent_profile, item_latent)
                
                # –î–æ–¥–∞—î–º–æ bias terms
                bias_corrected_prediction = (
                    self.global_mean + 
                    (user_mean - self.global_mean) + 
                    (self.item_means[item_idx] - self.global_mean) + 
                    raw_prediction
                )
                
                predicted_ratings.append((item_idx, bias_corrected_prediction, raw_prediction))
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–º–∏ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏
            predicted_ratings.sort(key=lambda x: x[1], reverse=True)
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç—Ä–µ–∫–∏ —è–∫—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–∂–µ —Å–ª—É—Ö–∞–≤
            user_interactions = self.data_loader.load_user_interactions()
            user_tracks = user_interactions[user_interactions['UserId'] == user_id]['SpotifyTrackId'].unique()
            listened_tracks = set(user_tracks)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
            recommendations = []
            song_features = self.data_loader.load_song_features()
            features_dict = song_features.set_index('SpotifyTrackId').to_dict('index')
            
            for item_idx, predicted_rating, raw_score in predicted_ratings:
                if len(recommendations) >= limit:
                    break
                
                item_id = self.idx_to_item_id[item_idx]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç—Ä–µ–∫–∏ —è–∫—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–∂–µ —Å–ª—É—Ö–∞–≤
                if item_id in listened_tracks:
                    continue
                
                # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–π—Ç–∏–Ω–≥ –¥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É 1-5
                normalized_rating = max(1.0, min(5.0, predicted_rating))
                
                track_info = features_dict.get(item_id, {})
                
                # –û–±—á–∏—Å–ª—é—î–º–æ confidence –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ª–∞—Ç–µ–Ω—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤
                user_norm = np.linalg.norm(user_latent_profile)
                item_norm = np.linalg.norm(self.svd_item_factors[item_idx])
                if user_norm > 0 and item_norm > 0:
                    latent_similarity = np.dot(user_latent_profile, self.svd_item_factors[item_idx]) / (user_norm * item_norm)
                    confidence = min(0.95, abs(latent_similarity) + 0.3)
                else:
                    confidence = 0.5
                
                recommendation = {
                    'track_id': item_id,
                    'title': track_info.get('Title', 'Unknown Track'),
                    'artist': track_info.get('Artist', 'Unknown Artist'),
                    'genre': track_info.get('Genre', 'Unknown Genre'),
                    'Title': track_info.get('Title', 'Unknown Track'),
                    'Artist': track_info.get('Artist', 'Unknown Artist'),
                    'Genre': track_info.get('Genre', 'Unknown Genre'),
                    'predicted_rating': float(normalized_rating),
                    'reason': f'SVD: –º–∞—Ç—Ä–∏—á–Ω–∞ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –∑ {self.svd_model.n_components} —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏',
                    'algorithm': 'Improved_SVD',
                    'raw_svd_score': float(raw_score),
                    'bias_corrected_prediction': float(predicted_rating),
                    'confidence': float(confidence),
                    'latent_similarity': float(latent_similarity) if 'latent_similarity' in locals() else 0.0,
                    'user_bias': float(user_mean - self.global_mean),
                    'item_bias': float(self.item_means[item_idx] - self.global_mean),
                    'global_mean': float(self.global_mean),
                    'matrix_factorization': True
                }
                
                recommendations.append(recommendation)
            
            logger.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(recommendations)} –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö SVD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π")
            return recommendations
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö SVD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π: {e}")
            return self._get_popular_items_fallback(limit)
    
    def _get_popular_items_fallback(self, limit: int) -> List[Dict]:
        """Fallback —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ"""
        try:
            logger.info("üîÑ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ fallback –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ")
            
            song_features = self.data_loader.load_song_features()
            user_interactions = self.data_loader.load_user_interactions()
            
            # –û–±—á–∏—Å–ª—é—î–º–æ –ø–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å —Ç—Ä–µ–∫—ñ–≤
            track_popularity = user_interactions.groupby('SpotifyTrackId').agg({
                'Rating': ['count', 'mean']
            }).reset_index()
            
            track_popularity.columns = ['SpotifyTrackId', 'rating_count', 'avg_rating']
            track_popularity = track_popularity[track_popularity['rating_count'] >= 2]
            track_popularity = track_popularity.sort_values(['avg_rating', 'rating_count'], ascending=False)
            
            recommendations = []
            features_dict = song_features.set_index('SpotifyTrackId').to_dict('index')
            
            for _, row in track_popularity.head(limit).iterrows():
                track_id = row['SpotifyTrackId']
                track_info = features_dict.get(track_id, {})
                
                recommendation = {
                    'track_id': track_id,
                    'title': track_info.get('Title', 'Unknown Track'),
                    'artist': track_info.get('Artist', 'Unknown Artist'),
                    'genre': track_info.get('Genre', 'Unknown Genre'),
                    'Title': track_info.get('Title', 'Unknown Track'),
                    'Artist': track_info.get('Artist', 'Unknown Artist'),
                    'Genre': track_info.get('Genre', 'Unknown Genre'),
                    'predicted_rating': float(row['avg_rating']),
                    'reason': f'–ü–æ–ø—É–ª—è—Ä–Ω–∏–π —Ç—Ä–µ–∫ (mid={row["avg_rating"]:.2f}, n={row["rating_count"]})',
                    'algorithm': 'Popularity_Fallback',
                    'rating_count': int(row['rating_count']),
                    'confidence': 0.7
                }
                
                recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ fallback —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π: {e}")
            return []
    
    def get_hybrid_recommendations(self, user_id: int, limit: int = 20) -> List[Dict]:
        """–ì—ñ–±—Ä–∏–¥–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó (–∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è content-based + collaborative + SVD)"""
        logger.info(f"üîÄ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≥—ñ–±—Ä–∏–¥–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ —É—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π (–±—ñ–ª—å—à–µ —Ç—Ä–µ–∫—ñ–≤ –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ)
        content_recs = self.get_content_recommendations(user_id, limit * 3)
        collaborative_recs = self.get_collaborative_recommendations(user_id, limit * 3)
        svd_recs = self.get_svd_recommendations(user_id, limit * 3)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–π –ø—É–ª —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
        combined_scores = {}
        
        # Content-Based: —Ñ–æ–∫—É—Å –Ω–∞ –∞—É–¥—ñ–æ —Å—Ö–æ–∂–æ—Å—Ç—ñ (40%)
        for i, rec in enumerate(content_recs):
            track_id = rec['track_id']
            # –î–∞—î–º–æ –±—ñ–ª—å—à–∏–π –≤–∞–≥–∞ –ø–µ—Ä—à–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è–º
            weight = 0.4 * (1.0 - i * 0.01)  # –∑–º–µ–Ω—à—É—î–º–æ –≤–∞–≥—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ
            combined_scores[track_id] = {
                'score': rec['predicted_rating'] * weight,
                'info': rec,
                'methods': ['content'],
                'diversity_bonus': 0.1  # –±–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç-—Å—Ö–æ–∂—ñ—Å—Ç—å
            }
        
        # Collaborative: —Ñ–æ–∫—É—Å –Ω–∞ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞—Ö (30%)
        for i, rec in enumerate(collaborative_recs):
            track_id = rec['track_id']
            weight = 0.3 * (1.0 - i * 0.01)
            if track_id in combined_scores:
                combined_scores[track_id]['score'] += rec['predicted_rating'] * weight
                combined_scores[track_id]['methods'].append('collaborative')
                combined_scores[track_id]['diversity_bonus'] += 0.15  # –±—ñ–ª—å—à–∏–π –±–æ–Ω—É—Å –∑–∞ –º–Ω–æ–∂–∏–Ω–Ω—ñ –º–µ—Ç–æ–¥–∏
            else:
                combined_scores[track_id] = {
                    'score': rec['predicted_rating'] * weight,
                    'info': rec,
                    'methods': ['collaborative'],
                    'diversity_bonus': 0.05
                }
        
        # SVD: —Ñ–æ–∫—É—Å –Ω–∞ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ (30%)
        for i, rec in enumerate(svd_recs):
            track_id = rec['track_id']
            weight = 0.3 * (1.0 - i * 0.01)
            if track_id in combined_scores:
                combined_scores[track_id]['score'] += rec['predicted_rating'] * weight
                combined_scores[track_id]['methods'].append('svd')
                combined_scores[track_id]['diversity_bonus'] += 0.2  # –Ω–∞–π–±—ñ–ª—å—à–∏–π –±–æ–Ω—É—Å –∑–∞ SVD
            else:
                combined_scores[track_id] = {
                    'score': rec['predicted_rating'] * weight,
                    'info': rec,
                    'methods': ['svd'],
                    'diversity_bonus': 0.1
                }
        
        # –î–æ–¥–∞—î–º–æ –±–æ–Ω—É—Å –∑–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å
        for track_id, data in combined_scores.items():
            data['final_score'] = data['score'] + data['diversity_bonus']
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º —Å–∫–æ—Ä–æ–º
        sorted_combined = sorted(
            combined_scores.items(),
            key=lambda x: x[1]['final_score'],
            reverse=True
        )[:limit]
        
        # –§–æ—Ä–º–∞—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é
        result = []
        for track_id, data in sorted_combined:
            info = data['info'].copy()
            info['predicted_rating'] = float(data['final_score'])
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å –ø—Ä–∏—á–∏–Ω–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
            methods = data['methods']
            if len(methods) == 1:
                method_desc = {
                    'content': 'Content-Based: —Å—Ö–æ–∂—ñ –∞—É–¥—ñ–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏',
                    'collaborative': 'Collaborative: —Å—Ö–æ–∂—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ',
                    'svd': 'SVD: –ª–∞—Ç–µ–Ω—Ç–Ω—ñ –º—É–∑–∏—á–Ω—ñ —Ñ–∞–∫—Ç–æ—Ä–∏'
                }
                info['reason'] = method_desc.get(methods[0], 'Hybrid')
            else:
                info['reason'] = f"Hybrid: {' + '.join(methods)} ({len(methods)} –º–µ—Ç–æ–¥–∏)"
            
            info['methods_used'] = methods
            info['diversity_score'] = float(data['diversity_bonus'])
            result.append(info)
        
        logger.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(result)} –≥—ñ–±—Ä–∏–¥–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π")
        return result
    
    def save_models(self, path: str = "models/"):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        os.makedirs(path, exist_ok=True)
        
        if self.content_model:
            joblib.dump(self.content_model, f"{path}/content_model.pkl")
            joblib.dump(self.scaler, f"{path}/scaler.pkl")
        
        if self.collaborative_model:
            joblib.dump(self.collaborative_model, f"{path}/collaborative_model.pkl")
            joblib.dump(self.user_item_matrix, f"{path}/user_item_matrix.pkl")
        
        if self.svd_model:
            joblib.dump(self.svd_model, f"{path}/svd_model.pkl")
            joblib.dump(self.user_item_matrix_normalized, f"{path}/user_item_matrix_normalized.pkl")
            joblib.dump(self.user_means, f"{path}/user_means.pkl")
            joblib.dump(self.item_means, f"{path}/item_means.pkl")
            joblib.dump(self.global_mean, f"{path}/global_mean.pkl")
            joblib.dump(self.svd_user_factors, f"{path}/svd_user_factors.pkl")
            joblib.dump(self.svd_item_factors, f"{path}/svd_item_factors.pkl")
        
        logger.info(f"üíæ –ú–æ–¥–µ–ª—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {path}")
    
    def load_models(self, path: str = "models/"):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            if os.path.exists(f"{path}/content_model.pkl"):
                self.content_model = joblib.load(f"{path}/content_model.pkl")
                self.scaler = joblib.load(f"{path}/scaler.pkl")
            
            if os.path.exists(f"{path}/collaborative_model.pkl"):
                self.collaborative_model = joblib.load(f"{path}/collaborative_model.pkl")
                self.user_item_matrix = joblib.load(f"{path}/user_item_matrix.pkl")
            
            if os.path.exists(f"{path}/svd_model.pkl"):
                self.svd_model = joblib.load(f"{path}/svd_model.pkl")
                self.user_item_matrix_normalized = joblib.load(f"{path}/user_item_matrix_normalized.pkl")
                self.user_means = joblib.load(f"{path}/user_means.pkl")
                self.item_means = joblib.load(f"{path}/item_means.pkl")
                self.global_mean = joblib.load(f"{path}/global_mean.pkl")
                self.svd_user_factors = joblib.load(f"{path}/svd_user_factors.pkl")
                self.svd_item_factors = joblib.load(f"{path}/svd_item_factors.pkl")
            
            self.is_trained = True
            logger.info(f"üì• –ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {path}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π: {e}")
            return False 